# Automation Stack Deployment - Complete Guide

Deploy entire automation environments with one click using Portainer or AWS.

## ğŸ¯ What Are Automation Stacks?

**Pre-configured bundles** of services that work together:
- ğŸ™ï¸ **LiveKit Voice Agent** - Complete voice AI with STT/LLM/TTS
- âš™ï¸ **Temporal Worker** - Workflow orchestration
- ğŸš€ **Complete Platform** - Everything combined

**Instead of manually deploying 7+ containers, just:**
1. Select a stack template
2. Choose deployment target (Docker or AWS)
3. Click "Deploy Stack"
4. Everything auto-configures and connects!

---

## ğŸ“¦ Available Stack Templates

### 1. **LiveKit Voice Agent (French)** ğŸ™ï¸

**What it deploys:**
```
7 Containers:
â”œâ”€â”€ LiveKit Server (WebRTC) - Port 7880
â”œâ”€â”€ Redis (Config storage) - Port 6379
â”œâ”€â”€ Ollama (Mistral LLM) - Port 11434
â”œâ”€â”€ Whisper (French STT) - Port 8001
â”œâ”€â”€ Piper (French TTS) - Port 8002
â”œâ”€â”€ Voice Agent (Python) - Background
â””â”€â”€ Web UI (Frontend) - Port 3001
```

**Requirements:**
- 16GB RAM
- 24GB VRAM (GPU)
- 8 CPU cores
- 50GB disk

**Best For:**
- Voice AI applications
- French language support
- Customer service bots
- Voice assistants

**Deployment Options:**
- âœ… Docker/Portainer (your GPU server)
- âœ… AWS Spot (g5.xlarge ~$0.30/hr)

### 2. **Temporal Worker** âš™ï¸

**What it deploys:**
```
1 Container:
â””â”€â”€ Temporal Worker
    â”œâ”€â”€ Your workflows registered
    â”œâ”€â”€ Activity implementations
    â””â”€â”€ Connects to existing Temporal server
```

**Requirements:**
- 4GB RAM
- 2 CPU cores
- 10GB disk
- No GPU needed

**Best For:**
- Workflow automation
- Adding workers to existing Temporal
- Scaling workflow execution

**Deployment Options:**
- âœ… Docker/Portainer only

### 3. **Complete Automation Platform** ğŸš€

**What it deploys:**
```
All LiveKit components + Temporal Worker
â”œâ”€â”€ Complete voice AI stack (7 containers)
â””â”€â”€ Temporal worker

Total: 8 containers, fully integrated
```

**Requirements:**
- 32GB RAM
- 24GB VRAM (GPU)
- 16 CPU cores
- 100GB disk

**Best For:**
- Production deployments
- All-in-one automation
- High-traffic applications

**Deployment Options:**
- âœ… AWS Spot only (g5.2xlarge ~$0.36/hr)

---

## ğŸš€ Deployment Flow

### **Example: Deploy LiveKit Voice Agent to Docker**

```
1. Go to /ai-compute/stacks

2. See available templates:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ğŸ™ï¸ LiveKit Voice Agent (French) â”‚
   â”‚ Complete voice AI with STT/LLM   â”‚
   â”‚                                  â”‚
   â”‚ Components: 7                    â”‚
   â”‚ Requires: 16GB RAM, 24GB VRAM    â”‚
   â”‚ [ğŸš€ Deploy Stack]                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Click "Deploy Stack"

4. Configure deployment:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Deploy: LiveKit Voice Agent      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Name: [Production Voice Bot]     â”‚
   â”‚                                  â”‚
   â”‚ Deploy To:                       â”‚
   â”‚ (â€¢) Docker/Portainer             â”‚
   â”‚     â””â”€ [prod-gpu-1 â–¼]           â”‚
   â”‚ ( ) AWS Spot Instance            â”‚
   â”‚                                  â”‚
   â”‚ Configuration:                   â”‚
   â”‚ LLM Model: [mistral â–¼]          â”‚
   â”‚ Whisper: [large-v3 â–¼]           â”‚
   â”‚ Voice: [fr_FR-siwis â–¼]          â”‚
   â”‚                                  â”‚
   â”‚ [ğŸš€ Deploy Stack]                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. System deploys:
   â³ Creating Docker network
   â³ Pulling images (7 images)
   â³ Creating containers
   â³ Starting services
   â³ Pulling Mistral model (4GB)
   â³ Configuring Redis
   â³ Running health checks
   âœ“ All services healthy!

6. Stack is ready:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Production Voice Bot   âœ“ Running â”‚
   â”‚                                  â”‚
   â”‚ Components: 7/7 running          â”‚
   â”‚ Host: 192.168.1.100              â”‚
   â”‚                                  â”‚
   â”‚ Access URLs:                     â”‚
   â”‚ â€¢ LiveKit: http://....:7880      â”‚
   â”‚ â€¢ Ollama: http://....:11434      â”‚
   â”‚ â€¢ Web UI: http://....:3001       â”‚
   â”‚                                  â”‚
   â”‚ LLM Connection Created:          â”‚
   â”‚ â€¢ "Production Voice Bot (Ollama)"â”‚
   â”‚   use_in_livekit: âœ“              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

7. Use it:
   - Open Web UI: http://192.168.1.100:3001
   - Start talking to French voice agent
   - Or use LLM connection in other apps
```

---

## ğŸ¨ Complete UI Flow

### **AI Compute Dashboard:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Compute                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Stats: Active Instances, Cost, GPU Hours]     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ LLM Deployment                              â”‚ â”‚
â”‚ â”‚ â”œâ”€ AWS Spot Instances                       â”‚ â”‚
â”‚ â”‚ â””â”€ Docker/Portainer                         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ†• Automation Stacks                        â”‚ â”‚
â”‚ â”‚                                             â”‚ â”‚
â”‚ â”‚ Deploy complete automation environments     â”‚ â”‚
â”‚ â”‚                                             â”‚ â”‚
â”‚ â”‚ [ğŸ™ï¸ LiveKit] [âš™ï¸ Temporal] [ğŸš€ Complete]   â”‚ â”‚
â”‚ â”‚                                             â”‚ â”‚
â”‚ â”‚ [Browse Stacks â†’]                           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [LLM Connections] [General Settings]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ What Happens During Deployment

### **Docker Deployment (via Portainer):**

```python
1. Generate docker-compose.yml from template
2. Apply user config overrides
3. Create Docker network
4. Deploy each container via Portainer API:
   - Pull images
   - Create containers with proper config
   - Set up port mappings
   - Configure environment variables
   - Add GPU support (if needed)
   - Set restart policies
5. Start all containers
6. Run post-deploy commands:
   - Pull LLM models
   - Initialize databases
   - Configure services
7. Create LLM connections
8. Run health checks
9. Mark as deployed!
```

### **AWS Deployment:**

```python
1. Generate docker-compose.yml
2. Generate AWS user-data script:
   - Install Docker
   - Install Docker Compose
   - Install NVIDIA drivers (if GPU)
   - Write docker-compose.yml to /opt/
   - Pull images and start
3. Launch AWS Spot instance
4. User-data runs on boot
5. All containers start automatically
6. Create LLM connections
7. Return access URLs
```

---

## ğŸ’¡ Auto-Configuration Features

### **Auto-Created LLM Connections:**

When stack deploys, system automatically creates:

```
LLM Connection: "Production Voice Bot (Ollama)"
â”œâ”€â”€ Type: Remote
â”œâ”€â”€ Base URL: http://192.168.1.100:11434
â”œâ”€â”€ Model: mistral
â”œâ”€â”€ Status: Active
â”œâ”€â”€ use_in_livekit: âœ“
â”œâ”€â”€ use_in_mcp: âœ“
â””â”€â”€ use_in_automation: âœ“

Now available in:
âœ“ LiveKit agents (can use this LLM)
âœ“ MCP servers (as a tool)
âœ“ Automation workflows (as an action)
```

### **Auto-Configuration:**

```
âœ“ Docker network created
âœ“ Port mappings configured
âœ“ Environment variables set
âœ“ Volume mounts created
âœ“ Dependencies ordered (depends_on)
âœ“ GPU access configured
âœ“ Health checks enabled
âœ“ Restart policies set
```

---

## ğŸ“Š Stack Management

### **View Deployed Stacks:**

```
/ai-compute/stacks â†’ "Deployed" tab

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production Voice Bot              âœ“ Running  â”‚
â”‚                                              â”‚
â”‚ Target: Docker (prod-gpu-1)                  â”‚
â”‚ Components: 7/7 running                      â”‚
â”‚ Deployed: Dec 15, 2025                       â”‚
â”‚                                              â”‚
â”‚ Access URLs:                                 â”‚
â”‚ â€¢ livekit: http://192.168.1.100:7880        â”‚
â”‚ â€¢ web: http://192.168.1.100:3001            â”‚
â”‚ â€¢ ollama: http://192.168.1.100:11434        â”‚
â”‚                                              â”‚
â”‚ [â–¶ï¸] [â¹ï¸] [ğŸ—‘ï¸]                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Use Cases

### **Use Case 1: Quick Voice Agent Testing**

```
Problem: Need to test French voice agent quickly

Solution:
1. Deploy "LiveKit Voice Agent" to local Docker
2. Select "medium" models for speed
3. Deploy in 5 minutes
4. Test immediately at http://localhost:3001
5. Terminate when done
```

### **Use Case 2: Production Voice Agent**

```
Problem: Need production-ready voice agent

Solution:
1. Deploy "LiveKit Voice Agent" to AWS g5.xlarge
2. Select "large-v3" for accuracy
3. Auto-configures everything
4. Access at public IP
5. Auto-creates LLM connection
6. Use connection in other apps too!
```

### **Use Case 3: Temporal Worker for Workflows**

```
Problem: Need to run Temporal workflows

Solution:
1. Deploy "Temporal Worker" to Docker
2. Configure Temporal server address
3. Worker registers your workflows
4. Start executing workflows
5. Visual timeline shows approvals
```

### **Use Case 4: Complete Platform on AWS**

```
Problem: Need everything for automation

Solution:
1. Deploy "Complete Automation Platform"
2. Select AWS g5.2xlarge (powerful instance)
3. Everything deploys: LiveKit + Temporal + LLM
4. Full automation environment ready
5. Pay ~$0.36/hour, terminate when done
```

---

## ğŸ“ Files Created:

```
Backend:
services/ai/api/app/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ stacks.py                    âœ¨ Stack models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ stack_templates.py           âœ¨ Pre-built templates
â”‚   â””â”€â”€ stack_deployer.py            âœ¨ Deployment logic
â””â”€â”€ routes/
    â””â”€â”€ stacks.py                    âœ¨ API endpoints

Frontend:
apps/web/app/(dashboard)/ai-compute/
â””â”€â”€ stacks/
    â””â”€â”€ page.tsx                     âœ¨ Stack browser & deployer

Documentation:
services/ai/
â””â”€â”€ AUTOMATION_STACKS_GUIDE.md       âœ¨ This file
```

---

## ğŸ”Œ API Endpoints

```
GET    /api/stacks/templates              List available templates
GET    /api/stacks/templates/{id}         Get template details
POST   /api/stacks/deploy                 Deploy a stack
GET    /api/stacks                        List deployed stacks
GET    /api/stacks/{id}                   Get deployment details
DELETE /api/stacks/{id}                   Terminate stack
GET    /api/stacks/{id}/docker-compose    Get docker-compose.yml
```

---

## ğŸ¨ Complete System Navigation

```
AI Compute
â”‚
â”œâ”€â”€ LLM Deployment
â”‚   â”œâ”€â”€ AWS Spot Instances
â”‚   â””â”€â”€ Docker/Portainer
â”‚
â”œâ”€â”€ ğŸ†• Automation Stacks
â”‚   â”œâ”€â”€ Available Stacks
â”‚   â”‚   â”œâ”€â”€ LiveKit Voice Agent
â”‚   â”‚   â”œâ”€â”€ Temporal Worker
â”‚   â”‚   â””â”€â”€ Complete Platform
â”‚   â””â”€â”€ Deployed Stacks
â”‚       â””â”€â”€ Manage running stacks
â”‚
â”œâ”€â”€ LLM Connections
â”‚   â””â”€â”€ (Auto-created by stacks!)
â”‚
â””â”€â”€ Settings
```

---

## âœ¨ Key Benefits

âœ… **One-Click Deployment** - No manual Docker/AWS setup
âœ… **Pre-Configured** - Everything works together out of the box
âœ… **Auto-Integration** - LLM connections created automatically
âœ… **Flexible** - Deploy to Docker or AWS
âœ… **Customizable** - Override models, voices, settings
âœ… **Production-Ready** - Includes health checks, restart policies
âœ… **Cost-Effective** - Use Docker (free) or AWS Spot (cheap)

---

## ğŸš€ Quick Start

### **1. Access Stacks Page:**
```
http://localhost:3000/ai-compute/stacks
```

### **2. Deploy Your First Stack:**
- Click on "LiveKit Voice Agent (French)"
- Choose Docker or AWS
- Configure settings
- Click "Deploy Stack"
- Wait 5-10 minutes
- Access web UI and start talking!

### **3. Use the LLM Connection:**
The deployed Ollama is now available as an LLM connection:
- Use in LiveKit agents
- Use in MCP servers
- Use in automation workflows

---

## ğŸ¯ Perfect For:

âœ… **Quick Testing** - Deploy stack, test, terminate
âœ… **Development** - Local Docker deployments
âœ… **Production** - AWS with auto-scaling
âœ… **Demos** - Show working system in minutes
âœ… **Learning** - See how components connect

---

Your automation infrastructure deployment is now as easy as deploying a single LLM! ğŸ‰
